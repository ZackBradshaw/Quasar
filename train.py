# -*- coding: utf-8 -*-
"""Finetuning_Mistral_7b_4bit_core_sna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17L3hoNGtCdgRTq4JU4koKHF6gvQmv9Pd

# Fine-tune Mistral


This code notebook makes it possible to fine-tune Mistral on a set of literary annotations.

## Import of the model

While it's possible to do it straight with the huggingface import functions, as I use  colab and drive, I prefer to import each file individually (espacially less issue with .cache)
"""

"""

!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/added_tokens.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/generation_config.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/pytorch_model-00001-of-00002.bin
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/pytorch_model-00002-of-00002.bin
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/special_tokens_map.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/tokenizer.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/tokenizer.model
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/tokenizer_config.json
!wget https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/pytorch_model.bin.index.json
"""
# %cd ".."

"""## Install

We connect to the drive

We install transformers and peft separately from the latest version on Github. Otherwhise, you will miss key metadata for Mistral support.
"""

# !pip install git+https://github.com/huggingface/transformers.git
# !pip install git+https://github.com/huggingface/peft.git

# """We install the other extensions."""

# !pip install -q accelerate bitsandbytes trl guardrail-ml tensorboard
# !apt-get -qq install poppler-utils tesseract-ocr
# !pip install -q unstructured["local-inference"] pillow

"""We load the libraries"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
    LlamaTokenizerFast,
)
from peft import LoraConfig, PeftModel, get_peft_model
from trl import SFTTrainer

"""## Parameters

Pour plus de commodité, nous déposons l'ensemble des paramètres ici (à commenter plus en détail ultérieurement)
"""

# Base parameters
local_rank = -1
per_device_train_batch_size = 4
per_device_eval_batch_size = 1
gradient_accumulation_steps = 4
learning_rate = 2e-4  # I prefer a high lr for analytical llm, but to each its own
max_grad_norm = 0.3
weight_decay = 0.001
lora_alpha = 16
lora_dropout = 0.1
lora_r = 64
group_by_length = True
max_seq_length = 1024  # Enough for the default text size.

# The name of Mistral model
model_name = "mistral-7b-v0.1"

# Le name of the new model.
new_model_name = "mistral-7b-sna"

# The number of steps.
# I prefer this to the number of epochs (easier to manage and anticipate the time it takes to finetune)
max_steps = 1001

# Activate 4-bit precision base model loading
use_4bit = True

# Activate nested quantization for 4-bit base models
use_nested_quant = False

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4=
bnb_4bit_quant_type = "nf4"

# Number of training epochs
num_train_epochs = 1

# Enable fp16 training
fp16 = True

# Enable bf16 training
bf16 = False

# Use packing dataset creating
packing = False

# Enable gradient checkpointing
gradient_checkpointing = True

# Optimizer to use, original is paged_adamw_32bit
optim = "paged_adamw_32bit"

# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)
lr_scheduler_type = "constant"

# Fraction of steps to do a warmup for
warmup_ratio = 0.03

# Group sequences into batches with same length (saves memory and speeds up training considerably)
group_by_length = True

# Sauvegarde des steps (permet de faire redémarrer l'entraînement si le fine-tuning ne fonctionne pas)
save_steps = 100

# Log every X updates steps
logging_steps = 1

# The output directory where the model predictions and checkpoints will be written
output_dir = "./mistral-7b-sna"

# Load the entire model on the GPU 0
device_map = {"": 0}

# Visualize training
report_to = "tensorboard"

# Tensorboard logs
tb_log_dir = "./mistral-7b-sna/logs"

"""Nous téléchargeons le modèle (à n'executer qu'une fois)

Nous l'importons maintenant dans la session avec une configuration spéciale (4bit, bf16) pour accélérer le fine-tuning
"""

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print(
            "Your GPU supports bfloat16, you can accelerate training with the argument --bf16"
        )
        print("=" * 80)

model = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=device_map, quantization_config=bnb_config
)

model.config.use_cache = False
model.config.pretraining_tp = 1

"""We load the tokenizer and the peft configuration. Notice you have to specify the target modules as peft is not yet fully updated for Mistral.

Also using the llama fast tokenizer but not sure if this is the best idea…
"""

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    inference_mode=False,
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"],
)

tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True)
# tokenizer = LlamaTokenizerFast.from_pretrained(model_name, add_eos_token=True, from_slow=True)

# This is the fix for fp16 training
tokenizer.padding_side = "right"

"""## Dataset preparation

We are going to use our dataset of literary analysis of public domain novels as demo. The analyis relies on a mix of synthetic and manual annotations.
"""

# !wget https://huggingface.co/Pclanglais/Brahe/resolve/main/brahe_instructions.json

"""We load the data in a custom format"""

from datasets import load_dataset


def format_custom(sample):
    instruction = f"<s>Text: {sample['full_text']} \n\n### Analysis:\n\n"
    context = None
    response = f"{sample['analysis']}"
    # join all the parts together
    prompt = "".join([i for i in [instruction, context, response] if i is not None])
    return prompt


# template dataset to add prompt to each sample
def template_dataset(sample):
    sample["text"] = f"{format_custom(sample)}{tokenizer.eos_token}"
    return sample


# Loadng the dataset.
data_files = {"train": "brahe_instructions.json"}
dataset = load_dataset("json", data_files=data_files, split="train")

# Transformation du dataset pour utiliser le format guanaco
dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))
dataset

dataset[40]

"""## Fine-tuning

Nous lançons l'entraînement :
"""

tokenizer.pad_token = tokenizer.eos_token

torch.cuda.empty_cache()

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard",
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# trainer.train()
trainer.train(resume_from_checkpoint=True)

"""We save the model"""

model_to_save = (
    trainer.model.module if hasattr(trainer.model, "module") else trainer.model
)  # Take care of distributed/parallel training
model_to_save.save_pretrained(new_model_name)

"""We merge the model and the LORA to get inference speed up."""

del model
torch.cuda.empty_cache()

from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    new_model_name, device_map="auto", torch_dtype=torch.bfloat16
)
model = model.merge_and_unload()

output_merged_dir = os.path.join(new_model_name, "final_merged_checkpoint")
model.save_pretrained(output_merged_dir, safe_serialization=True)

"""We export the tokenizer files."""

# !cp "mistral-7b-v0.1/tokenizer.json" "mistral-7b-sna/final_merged_checkpoint/tokenizer.json"
# !cp "mistral-7b-v0.1/tokenizer.model" "mistral-7b-sna/final_merged_checkpoint/tokenizer.model"
# !cp "mistral-7b-v0.1/tokenizer_config.json" "mistral-7b-sna/final_merged_checkpoint/tokenizer_config.json"
# !cp "mistral-7b-v0.1/special_tokens_map.json" "mistral-7b-sna/final_merged_checkpoint/special_tokens_map.json"

"""## Inference

To do later: not working for now (but you're free to debug). You may need to delete the runtime to free the memory (since we will use a different implementation), especially if you are on the free colab.
"""

# !pip install git+https://github.com/huggingface/transformers.git

# !pip install git+https://github.com/vllm-project/vllm


# from vllm import LLM, SamplingParams
# import os

# new_model_name = "mistral-7b-sna"

# output_merged_dir = os.path.join(new_model_name, "final_merged_checkpoint")
# llm = LLM(output_merged_dir)

# sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=500)

# prompts = ["""Text: For a long time I used to go to bed early. Sometimes, when I had put out my candle, my eyes would close so quickly that I had not even time to say "I'm going to sleep." And half an hour later the thought that it was time to go to sleep would awaken me; I would try to put away the book which, I imagined, was still in my hands, and to blow out the light; I had been thinking all the time, while I was asleep, of what I had just been reading, but my thoughts had run into a channel of their own, until I myself seemed actually to have become the subject of my book: a church, a quartet, the rivalry between François I and Charles V. This impression would persist for some moments after I was awake; it did not disturb my mind, but it lay like scales upon my eyes and prevented them from registering the fact that the candle was no longer burning. Then it would begin to seem unintelligible, as the thoughts of a former existence must be to a reincarnate spirit; the subject of my book would separate itself from me, leaving me free to choose whether I would form part of it or no; and at the same time my sight would return and I would be astonished to find myself in a state of darkness, pleasant and restful enough for the eyes, and even more, perhaps, for my mind, to which it appeared incomprehensible, without a cause, a matter dark indeed. \n\n### Analysis:\n\n"""]

# prompts = ["""Text: grandfather’s, who died years ago; and my body, the side
# upon which I was lying, faithful guardians of a past which
# my mind should never have forgotten, brought back
# before my eyes the glimmering flame of the night-light in
# its urn-shaped bowl of Bohemian glass that hung by
# chains from the ceiling, and the chimney-piece of Siena
# marble in my bedroom at Combray, in my grandparents’
# house, in those far distant days which at this moment I
# imagined to be in the present without being able to picture
# them exactly, and which would become plainer in a little
# while when I was properly awake.
#       Then the memory of a new position would spring up,
# and the wall would slide away in another direction; I was
# in my room in Mme de Saint-Loup’s house in the country;
# good heavens, it must be ten o’clock, they will have
# finished dinner! I must have overslept myself in the little
# nap which I always take when I come in from my walk
# with Mme de Saint-Loup, before dressing for the evening.
# For many years have now elapsed since the Combray days
# when, coming in from the longest and latest walks, I
# would still be in time to see the reflection of the sunset
# glowing in the panes of my bedroom window. It is a very
# different kind of life that one leads at Tansonville, at Mme
# de Saint-Loup’s, and a different kind of pleasure that I
# derive from taking walks only in the evenings, from
# visiting by moonlight the roads on which I used to play as
# a child in the sunshine; as for the bedroom in which I must
# have fallen asleep instead of dressing for dinner, I can see
# it from the distance as we return from our walk, with its
# lamp shining through the window, a solitary beacon in the
# night.\n\n### Analysis:\n\n"""]

# outputs = llm.generate(prompts, sampling_params)

# outputs

# """# Application du modèle à un jeu de données"""

# import pandas as pd
# proust = pd.read_excel("proust_novel.xlsx")

# prompts = []

# for texts in proust["text"].tolist():
#   prompts.append("Text: " + texts + "\n\n### Analysis:\n\n")

# print(prompts[0])

# outputs = llm.generate(prompts, sampling_params)

# proust["analysis"] = outputs

# proust

# proust.to_excel("proust_novel_mistral_3.xlsx")
